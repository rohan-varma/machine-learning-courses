{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Softmax/SVM Compare and Contrast\n",
    "\n",
    "- Softmax provides interpretation of the scores that our classifier outputs\n",
    "- Interpret normalized scores as a probability distribution over the labels. From an information theory perspective, we're trying to minimize the KL divergence between 2 probability distributions - one is the true label distribution where all the mass is concentrated over the right label, and the other is the distribution output by our softmax scoring function\n",
    "- SVM is satisfied once max margin property is satisfied. However softmax is never really satisfied unless the probability of the correct label is 1.0 and the others are 0. \n",
    "- Example: the correct label is 0 and the label scores are [10, 9, 9] or [10, -100, -100]. With a margin of 1, the SVM doesn't really differentiate between these 2, the loss is the same (0), since 9-10 + 1 = 0 -> loss is 0. \n",
    "- But softmax would incur a much higher loss for the first set of scores than the second. \n",
    "- This can have positives and negatives. For SVMs for example, if we're trying to learn how to correctly classify cars, then we should spend a lot of our effort into correctly distinguishing between trucks and cars (i.e. maximizing that decision margin), and if we've achieved that then we're pretty sure our classifier should have no trouble distinguishing between say a frog and a car. On the other hand, a softmax classifier would spend equal effort trying to separate between a frog and a car and a truck and a car. \n",
    "- The pro for softmax classification is that it's not just enough to satisfy the margin and have the correct label, the classifier needs to be pretty confident that it's not wrong as well. So the incorrect classes need to have low probability. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Softmax Run down\n",
    "\n",
    "- Given a score vector $s$ from our model, the softmax function computes $\\frac{e^{s_i}}{\\sum_{i=1}^{N}e^{s_i}}$\n",
    "- Remember numeric stability, push a constant $max(s)$ into the exponentiation to avoid overflow\n",
    "- Loss is given by $L_i = -log p(Y = y_i | X = x_i)$, basically take the negative log of the score of the **correct** label. \n",
    "- Why does this make sense? log(x) is increasing on the interval $(0, 1)$ so $-log(x)$ is decreasing on that interval. So if we have a score of 0.8 for the correct label, our loss will be 0.09, if we have a score of -.08 our loss would be 1.09.\n",
    "- Common pitfall: taking the negative log of the **predicted** label and thinking of that as the loss. This will lead us to try to maximize whichever is the predicted label at each iteration, which is incorrect!\n",
    "- Bounds of this loss function is 0 and infinity. \n",
    "- Initially, this loss should be around $log C$. To see this, assume that since we uniformly randomly initialize our weights to be around 0, each class scores will initially be around 0. And looking at $\\frac{e^{s_i}}{\\sum_{i=1}^{N}e^{s_i}}$ we'll have basically -log(1/c) as our loss.\n",
    "\n",
    "### More details on cross entropy\n",
    "- Overall loss given by $- \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_i \\log(h_{\\theta}(x_i))$, but the inner term is only activated once since we one-hot encode the y, so we only get the negative log across all of the correct labels for the dataset.\n",
    "- Another variant on the cross entropy loss also adds the other predicted class scores to the loss: $- \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_i \\log(h_{\\theta}(x_i)) + (1-y_j)log(1 - h_{\\theta}(x_i))$. The 1 - operations are applied element wise to the vector.\n",
    "- Basically here, the second term inverts our labelling, and considers the other predicted scores as the predicted classes, giving then probabilities of $1 - s_i$, and adding them to the penalty terms. \n",
    "- This second version more closely ties into the MLE, but the first version appears to be more common\n",
    "- [This code](https://github.com/rohan-varma/machine-learning-courses/blob/master/cs231n/loss.py#L72) implements the softmax loss function, and shows a plot of how the alternate version (that penalizes all the predictions instead of only the correct class score) differs from the more usual cross-entropy loss function (it turns out it's just shifted up). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
